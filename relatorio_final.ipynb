{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pós-Graduação Strictu Sensu em Ciências da Computação\n",
    "----\n",
    "**Universidade Federal de Pernambuco**\n",
    "\n",
    "**Disciplina**: Aprendizado de Máquina\n",
    "\n",
    "**Discentes**: Jose Adeljan Marinho da Silva\n",
    "\n",
    "Matheus Hopper Jansen Costa\n",
    "\n",
    "Heitor Leite Ramos\n",
    "\n",
    "Jefferson Medeiros Norberto\n",
    "\n",
    "Karoline Juliana Costa da Silva\n",
    "\n",
    "**Docentes**: Leandro Maciel Almeida\n",
    "\n",
    "Francisco de Assis Tenorio de Carvalho\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário:\n",
    "\n",
    "0. [Importação das bibliotecas](#importação-das-bibliotecas-utilizadas)\n",
    "1. [Introdução](#introdução)\n",
    "2. [Análise Exploratória dos Dados](#análise-exploratória-dos-dados)\n",
    "3. [Preparação dos dados](#preparação-dos-dados)\n",
    "4. [Modelagem e Otimização](#modelagem-e-otimização)\n",
    "5. [Avaliação dos Modelos](#avaliação-dos-modelos)\n",
    "6. [Análise de Custo-Benefício](#análise-de-custo-benefício)\n",
    "7. [Teste de Estresse dos Modelos](#teste-de-estresse-dos-modelos)\n",
    "8. [Discussão sobre Limitações e Futuras Melhorias](#discussão-sobre-limitações-e-futuras-melhorias)\n",
    "9. [Conclusão](#conclusão)\n",
    "10. [Referências](#referências)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação das bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openml in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (2.5.0)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (0.14.2)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (2.2.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.6.2 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (2.1.3)\n",
      "Requirement already satisfied: minio in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (7.2.12)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (18.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (4.67.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from openml) (24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from pandas>=1.0.0->openml) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from pandas>=1.0.0->openml) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from python-dateutil->openml) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from scikit-learn>=0.18->openml) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from scikit-learn>=0.18->openml) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from minio->openml) (2024.8.30)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from minio->openml) (2.2.3)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from minio->openml) (23.1.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from minio->openml) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from minio->openml) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from requests->openml) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from requests->openml) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from tqdm->openml) (0.4.6)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\.conda\\envs\\venvmodel\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\.conda\\envs\\venvmodel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Modelos\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Validação e métricas\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Bibliotecas adicionais\n",
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import time\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(45069)\n",
    "\n",
    "# Carregar o dataset\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df_bruto = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação do target em binário\n",
    "df['class'] = [1 if each=='<30' else 0 for each in df['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação das colunas categóricas em string\n",
    "categorical_cols = df.select_dtypes(include='category').columns\n",
    "df[categorical_cols] = df[categorical_cols].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"race\"].fillna(df[\"race\"].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[~df.discharge_disposition_id.isin([11,18,19,20,21,7,25,26])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_list = ['diag_1','diag_2','diag_3']\n",
    "\n",
    "for col in diag_list:\n",
    "    df[col].fillna('NaN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformFunc(value):\n",
    "    value = re.sub(\"V[0-9]*\", \"0\", value) # V\n",
    "    value = re.sub(\"E[0-9]*\", \"0\", value) # E\n",
    "    value = re.sub('NaN', \"-1\", value) # Nan\n",
    "    return value\n",
    "\n",
    "def transformCategory(value):\n",
    "    if value>=390 and value<=459 or value==785:\n",
    "        category = 'Circulatory'\n",
    "    elif value>=460 and value<=519 or value==786:\n",
    "        category = 'Respiratory'\n",
    "    elif value>=520 and value<=579 or value==787:\n",
    "        category = 'Digestive'\n",
    "    elif value==250:\n",
    "        category = 'Diabetes'\n",
    "    elif value>=800 and value<=999:\n",
    "        category = 'Injury'\n",
    "    elif value>=710 and value<=739:\n",
    "        category = 'Musculoskeletal'\n",
    "    elif value>=580 and value<=629 or value==788:\n",
    "        category = 'Genitourinary'\n",
    "    elif value>=140 and value<=239 :\n",
    "        category = 'Neoplasms'\n",
    "    elif value==-1:\n",
    "        category = 'NAN'\n",
    "    else :\n",
    "        category = 'Other'\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in diag_list:\n",
    "    df[col] = df[col].apply(transformFunc)\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in diag_list:\n",
    "    df[col] = df[col].apply(transformCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['uint8', 'int64']).columns.tolist()\n",
    "numerical_columns.remove('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors = 2 , contamination = 0.1)\n",
    "clf.fit_predict(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11237244e+00, -1.00000000e+00, -1.05618622e+00, -1.00000000e+00,\n",
       "       -9.64101615e-01, -9.76153125e-01, -1.20628580e+00, -1.00000000e+00,\n",
       "       -9.54124145e-01, -1.00000000e+00, -1.35838165e+00, -1.06769362e+00,\n",
       "       -9.73606798e-01, -9.65428645e-01, -1.18383741e+00, -1.16313671e+00,\n",
       "       -1.21086536e+00, -5.00000000e+09, -1.41421356e+00, -1.13567449e+00,\n",
       "       -1.00000000e+00, -1.17157288e+00, -1.28656609e+00, -1.22474487e+00,\n",
       "       -1.10355339e+00, -1.31392974e+00, -1.02160050e+00, -1.19841474e+00,\n",
       "       -1.10102051e+00, -1.17157288e+00])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = clf.negative_outlier_factor_\n",
    "df_scores[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.31662479e+10, -2.64575131e+10, -2.23606798e+10, -2.23606798e+10,\n",
       "       -2.00000000e+10, -1.73205081e+10, -1.73205081e+10, -1.73205081e+10,\n",
       "       -1.73205081e+10, -1.73205081e+10, -1.73205081e+10, -1.73205081e+10,\n",
       "       -1.73205081e+10, -1.73205081e+10, -1.73205081e+10, -1.49767620e+10,\n",
       "       -1.41421356e+10, -1.41421356e+10, -1.41421356e+10, -1.41421356e+10,\n",
       "       -1.41421356e+10, -1.41421356e+10, -1.41421356e+10, -1.41421356e+10,\n",
       "       -1.41421356e+10, -1.41421356e+10, -1.41421356e+10, -1.41421356e+10,\n",
       "       -1.41421356e+10, -1.41421356e+10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(df_scores)[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value = np.sort(df_scores)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_tf = df_scores > threshold_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df_scores > threshold_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1Cresult and max_glu_serum\n",
    "df['A1Cresult'] = df['A1Cresult'].replace(['>7','>8','Norm','None'],[1,1,0,-99])\n",
    "df['max_glu_serum'] = df['max_glu_serum'].replace(['>200','>300','Norm','None'],[1,1,0,-99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot Encoding Race and Id's\n",
    "one_hot_data = pd.get_dummies(df, columns=['race'], prefix=[\"enc\"], drop_first = True)\n",
    "\n",
    "columns_ids = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "\n",
    "one_hot_data[columns_ids] = one_hot_data[columns_ids].astype('str')\n",
    "one_hot_data = pd.get_dummies(one_hot_data, columns=columns_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_cols = ['diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "df = pd.get_dummies(df, columns=diag_cols, prefix=[\"encdiag_1\", \"encdiag_2\", \"encdiag_3\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['encdiag_1_NAN', 'encdiag_2_NAN', 'encdiag_3_NAN']\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminando colunas com grande número de valores ausentes\n",
    "df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as linhas Unknown/Invalid da coluna gender\n",
    "df = df.loc[~df.gender.isin(['Unknown/Invalid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['citoglipton', 'examide'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numchange\n",
       "0    74056\n",
       "1    26272\n",
       "2     1318\n",
       "3      108\n",
       "4        5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "        'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "        'miglitol', 'insulin', 'glyburide.metformin', 'tolazamide', 'metformin.pioglitazone',\n",
    "        'metformin.rosiglitazone', 'glimepiride.pioglitazone', 'glipizide.metformin',\n",
    "        'troglitazone', 'tolbutamide', 'acetohexamide']\n",
    "for col in keys:\n",
    "    colname = str(col) + 'temp'\n",
    "    df[colname] = df[col].apply(lambda x: 0 if (x == 'No' or x == 'Steady') else 1)\n",
    "df['numchange'] = 0\n",
    "for col in keys:\n",
    "    colname = str(col) + 'temp'\n",
    "    df['numchange'] = df['numchange'] + df[colname]\n",
    "    del df[colname]\n",
    "\n",
    "df['numchange'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['change'] = df['change'].replace('Ch', 1)\n",
    "df['change'] = df['change'].replace('No', 0)\n",
    "df['gender'] = df['gender'].replace('Male', 1)\n",
    "df['gender'] = df['gender'].replace('Female', 0)\n",
    "df['diabetesMed'] = df['diabetesMed'].replace('Yes', 1)\n",
    "df['diabetesMed'] = df['diabetesMed'].replace('No', 0)\n",
    "\n",
    "for col in keys:\n",
    "    df[col] = df[col].replace('No', 0)\n",
    "    df[col] = df[col].replace('Steady', 1)\n",
    "    df[col] = df[col].replace('Up', 1)\n",
    "    df[col] = df[col].replace('Down', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nummed\n",
       "1    47312\n",
       "0    23401\n",
       "2    21871\n",
       "3     7777\n",
       "4     1335\n",
       "5       58\n",
       "6        5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nummed'] = 0\n",
    "\n",
    "for col in keys:\n",
    "    df['nummed'] = df['nummed'] + df[col]\n",
    "df['nummed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age\n",
       "8     26066\n",
       "7     22480\n",
       "6     17256\n",
       "9     17196\n",
       "5      9685\n",
       "4      3775\n",
       "10     2792\n",
       "3      1657\n",
       "2       691\n",
       "1       161\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformando os intervalos das idade em 1 - 10\n",
    "for i in range(0,10):\n",
    "    df['age'] = df['age'].replace('['+str(10*i)+'-'+str(10*(i+1))+')', i+1)\n",
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "8     26066\n",
      "7     22480\n",
      "6     17256\n",
      "9     17196\n",
      "5      9685\n",
      "4      3775\n",
      "10     2792\n",
      "3      1657\n",
      "2       691\n",
      "1       161\n",
      "Name: count, dtype: int64\n",
      "age\n",
      "75    26066\n",
      "65    22480\n",
      "55    17256\n",
      "85    17196\n",
      "45     9685\n",
      "35     3775\n",
      "95     2792\n",
      "25     1657\n",
      "15      691\n",
      "5       161\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['age'] = df['age'].astype('int64')\n",
    "print(df.age.value_counts())\n",
    "# convert age categories to mid-point values\n",
    "age_dict = {1:5, 2:15, 3:25, 4:35, 5:45, 6:55, 7:65, 8:75, 9:85, 10:95}\n",
    "df['age'] = df.age.map(age_dict)\n",
    "print(df.age.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionterms = [('num_medications','time_in_hospital'),\n",
    "('num_medications','num_procedures'),\n",
    "('time_in_hospital','num_lab_procedures'),\n",
    "('num_medications','num_lab_procedures'),\n",
    "('num_medications','number_diagnoses'),\n",
    "('age','number_diagnoses'),\n",
    "('change','num_medications'),\n",
    "('number_diagnoses','time_in_hospital'),\n",
    "('num_medications','numchange')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inter in interactionterms:\n",
    "    name = inter[0] + '|' + inter[1]\n",
    "    df[name] = df[inter[0]] * df[inter[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_medications</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_medications|time_in_hospital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_medications  time_in_hospital  num_medications|time_in_hospital\n",
       "0               17                 4                                68\n",
       "1               10                 3                                30\n",
       "2                8                 2                                16\n",
       "3               12                 1                                12\n",
       "4               23                 3                                69"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['num_medications','time_in_hospital', 'num_medications|time_in_hospital']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = df.select_dtypes(include=['bool']).columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
      "count      1.017590e+05        1.017590e+05    1.017590e+05     1.017590e+05   \n",
      "mean       5.586083e-18        8.281368e-17    1.927199e-17    -1.094872e-16   \n",
      "std        1.000005e+00        1.000005e+00    1.000005e+00     1.000005e+00   \n",
      "min       -1.137692e+00       -2.139670e+00   -7.853987e-01    -1.848262e+00   \n",
      "25%       -8.026939e-01       -6.148364e-01   -7.853987e-01    -7.409253e-01   \n",
      "50%       -1.326970e-01        4.592477e-02   -1.991639e-01    -1.257385e-01   \n",
      "75%        5.372998e-01        7.066860e-01    3.870708e-01     4.894483e-01   \n",
      "max        3.217287e+00        4.518770e+00    2.732010e+00     7.994727e+00   \n",
      "\n",
      "       number_outpatient  number_emergency  number_inpatient  \\\n",
      "count       1.017590e+05      1.017590e+05      1.017590e+05   \n",
      "mean        5.250918e-17     -6.703299e-18      2.122711e-17   \n",
      "std         1.000005e+00      1.000005e+00      1.000005e+00   \n",
      "min        -2.914724e-01     -2.126106e-01     -5.032979e-01   \n",
      "25%        -2.914724e-01     -2.126106e-01     -5.032979e-01   \n",
      "50%        -2.914724e-01     -2.126106e-01     -5.032979e-01   \n",
      "75%        -2.914724e-01     -2.126106e-01      2.885370e-01   \n",
      "max         3.284988e+01      8.146555e+01      1.612524e+01   \n",
      "\n",
      "       number_diagnoses        nummed  num_medications|time_in_hospital  \\\n",
      "count      1.017590e+05  1.017590e+05                      1.017590e+05   \n",
      "mean       1.368590e-16 -1.536173e-17                     -4.217492e-17   \n",
      "std        1.000005e+00  1.000005e+00                      1.000005e+00   \n",
      "min       -3.321707e+00 -1.282035e+00                     -1.167569e+00   \n",
      "25%       -7.357814e-01 -1.952875e-01                     -7.645749e-01   \n",
      "50%        2.985890e-01 -1.952875e-01                     -3.265377e-01   \n",
      "75%        8.157742e-01  8.914596e-01                      4.969722e-01   \n",
      "max        4.436071e+00  5.238448e+00                      3.300410e+00   \n",
      "\n",
      "       num_medications|num_procedures  time_in_hospital|num_lab_procedures  \\\n",
      "count                    1.017590e+05                         1.017590e+05   \n",
      "mean                     1.187043e-17                        -1.005495e-17   \n",
      "std                      1.000005e+00                         1.000005e+00   \n",
      "min                     -6.427330e-01                        -1.474467e+00   \n",
      "25%                     -6.427330e-01                        -8.682216e-01   \n",
      "50%                     -3.884493e-01                        -1.209884e-01   \n",
      "75%                      2.218314e-01                         7.813310e-01   \n",
      "max                      5.841500e+00                         2.120711e+00   \n",
      "\n",
      "       num_medications|num_lab_procedures  num_medications|number_diagnoses  \\\n",
      "count                        1.017590e+05                      1.017590e+05   \n",
      "mean                         2.848902e-17                      6.703299e-18   \n",
      "std                          1.000005e+00                      1.000005e+00   \n",
      "min                         -1.608920e+00                     -1.827313e+00   \n",
      "25%                         -8.896060e-01                     -7.696688e-01   \n",
      "50%                         -3.708542e-02                     -1.149369e-01   \n",
      "75%                          8.553971e-01                      7.412510e-01   \n",
      "max                          1.787842e+00                      2.453627e+00   \n",
      "\n",
      "       age|number_diagnoses  change|num_medications  \\\n",
      "count          1.017590e+05            1.017590e+05   \n",
      "mean          -1.312729e-16            3.184067e-17   \n",
      "std            1.000005e+00            1.000005e+00   \n",
      "min           -2.653939e+00           -7.779575e-01   \n",
      "25%           -6.586003e-01           -7.779575e-01   \n",
      "50%           -1.146354e-02           -7.779575e-01   \n",
      "75%            9.592416e-01            7.035918e-01   \n",
      "max            4.653314e+00            6.722386e+00   \n",
      "\n",
      "       number_diagnoses|time_in_hospital  num_medications|numchange  \n",
      "count                       1.017590e+05               1.017590e+05  \n",
      "mean                       -1.078114e-16              -7.038464e-17  \n",
      "std                         1.000005e+00               1.000005e+00  \n",
      "min                        -1.249960e+00              -5.038498e-01  \n",
      "25%                        -7.180871e-01              -5.038498e-01  \n",
      "50%                        -2.621962e-01              -5.038498e-01  \n",
      "75%                         4.216401e-01               3.170667e-01  \n",
      "max                         6.614158e+00               1.536720e+01  \n"
     ]
    }
   ],
   "source": [
    "# Colunas numéricas\n",
    "numeric_cols = [\n",
    "    'time_in_hospital',\n",
    "    'num_lab_procedures',\n",
    "    'num_procedures',\n",
    "    'num_medications',\n",
    "    'number_outpatient',\n",
    "    'number_emergency',\n",
    "    'number_inpatient',\n",
    "    'number_diagnoses',\n",
    "    'nummed',\n",
    "    'num_medications|time_in_hospital',\n",
    "    'num_medications|num_procedures',\n",
    "    'time_in_hospital|num_lab_procedures',\n",
    "    'num_medications|num_lab_procedures',\n",
    "    'num_medications|number_diagnoses',\n",
    "    'age|number_diagnoses',\n",
    "    'change|num_medications',\n",
    "    'number_diagnoses|time_in_hospital',\n",
    "    'num_medications|numchange'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição no conjunto de treino:\n",
      "class\n",
      "0    0.888396\n",
      "1    0.111604\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribuição no conjunto de validação:\n",
      "class\n",
      "0    0.888365\n",
      "1    0.111635\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribuição no conjunto de teste:\n",
      "class\n",
      "0    0.888414\n",
      "1    0.111586\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Tamanho do conjunto de treino: X_train=(61055, 125), y_train=(61055,)\n",
      "Tamanho do conjunto de validação: X_val=(20352, 125), y_val=(20352,)\n",
      "Tamanho do conjunto de teste: X_test=(20352, 125), y_test=(20352,)\n",
      "Tamanho treino: 60.00%\n",
      "Tamanho validação: 20.00%\n",
      "Tamanho teste: 20.00%\n",
      "\n",
      "Arquivos CSV salvos com sucesso:\n",
      "- train_data.csv\n",
      "- validation_data.csv\n",
      "- test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecionar as features e a variável target\n",
    "feature_set = [col for col in df.columns if col != 'class']\n",
    "X = df[feature_set]\n",
    "y = df['class']\n",
    "\n",
    "# Divisão treino (60%) e teste (20%) e validação (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Exibir as distribuições das classes em cada conjunto\n",
    "print(\"Distribuição no conjunto de treino:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribuição no conjunto de validação:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribuição no conjunto de teste:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Exibir os tamanhos de cada conjunto\n",
    "print(f\"\\nTamanho do conjunto de treino: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de validação: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "print(f\"Tamanho treino: {len(X_train) / len(X):.2%}\")\n",
    "print(f\"Tamanho validação: {len(X_val) / len(X):.2%}\")\n",
    "print(f\"Tamanho teste: {len(X_test) / len(X):.2%}\")\n",
    "\n",
    "# Salvar os conjuntos em arquivos CSV\n",
    "# Concatenar X_train com y_train\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "\n",
    "# Concatenar X_val com y_val\n",
    "validation_data = pd.concat([X_val, y_val], axis=1)\n",
    "validation_data.to_csv(\"validation_data.csv\", index=False)\n",
    "\n",
    "# Concatenar X_test com y_test\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nArquivos CSV salvos com sucesso:\")\n",
    "print(\"- train_data.csv\")\n",
    "print(\"- validation_data.csv\")\n",
    "print(\"- test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem e Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os conjuntos de dados de treinamento, validação e teste\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "validation_data = pd.read_csv('validation_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando features e target no conjunto de treinamento\n",
    "X_train = train_data.drop('class', axis=1)\n",
    "y_train = train_data['class']\n",
    "\n",
    "# Separando features e target no conjunto de validação\n",
    "X_val = validation_data.drop('class', axis=1)\n",
    "y_val = validation_data['class']\n",
    "\n",
    "# Separando features e target no conjunto de teste\n",
    "X_test = test_data.drop('class', axis=1)\n",
    "y_test = test_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição das classes no conjunto de treinamento após o ADASYN:\n",
      "Counter({0: 54241, 1: 53008})\n"
     ]
    }
   ],
   "source": [
    "# Aplicando o ADASYN no conjunto de treinamento\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train, y_train = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Distribuição das classes no conjunto de treinamento após o ADASYN:')\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliar o modelo utilizando validação cruzada estratificada com k=10\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    # Cálculo da ACSA\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    acsa = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "    print(f\"{model_name} - Acurácia: {acc:.4f}, F1-Score: {f1:.4f}, Recall: {recall:.4f}, ACSA: {acsa:.4f}\")\n",
    "    \n",
    "    # Plotando a matriz de confusão\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Matriz de Confusão - {model_name}')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimindo o relatório de classificação\n",
    "    print(f\"Relatório de Classificação para {model_name}:\\n\", classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Plotando a curva ROC, se possível\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_val)[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "        roc_auc = roc_auc_score(y_val, y_prob)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "        plt.plot([0,1], [0,1], 'k--')\n",
    "        plt.title(f'Curva ROC - {model_name}')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{model_name} não suporta predict_proba, curva ROC não será plotada.\")\n",
    "    \n",
    "    return acc, f1, recall, acsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os modelos\n",
    "# Dicionário para armazenar os modelos\n",
    "models = {\n",
    "    'K-NN': KNeighborsClassifier(),\n",
    "    'LVQ': NearestCentroid(),\n",
    "    'Árvore de Decisão': DecisionTreeClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Rede Neural MLP': MLPClassifier(max_iter=500),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Comitê de Redes Neurais Artificiais\n",
    "nn1 = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=500, random_state=1)\n",
    "nn2 = MLPClassifier(hidden_layer_sizes=(100,), activation='tanh', solver='sgd', max_iter=500, random_state=2)\n",
    "nn3 = MLPClassifier(hidden_layer_sizes=(50, 50), activation='relu', solver='adam', max_iter=500, random_state=3)\n",
    "\n",
    "committee_nn = VotingClassifier(estimators=[\n",
    "    ('nn1', nn1),\n",
    "    ('nn2', nn2),\n",
    "    ('nn3', nn3)\n",
    "], voting='soft')\n",
    "\n",
    "models['Comitê de Redes Neurais Artificiais'] = committee_nn\n",
    "\n",
    "# Comitê Heterogêneo (Stacking)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=final_estimator, cv=5)\n",
    "\n",
    "models['Comitê Heterogêneo (Stacking)'] = stacking_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 11:14:32,287] A new study created in memory with name: K-NN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimização para o modelo: K-NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 11:14:50,479] Trial 0 finished with value: 0.6984866944639428 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'algorithm': 'auto'}. Best is trial 0 with value: 0.6984866944639428.\n",
      "[I 2024-11-30 11:15:08,977] A new study created in memory with name: LVQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimização para o modelo: LVQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 11:15:11,655] Trial 0 finished with value: 0.6058612498819502 and parameters: {'metric': 'manhattan', 'use_shrinkage': True, 'shrink_threshold': 0.15601864044243652}. Best is trial 0 with value: 0.6058612498819502.\n",
      "[I 2024-11-30 11:15:15,362] A new study created in memory with name: Árvore de Decisão\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimização para o modelo: Árvore de Decisão\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 11:15:18,588] Trial 0 finished with value: 0.8017233767832863 and parameters: {'max_depth': 8, 'min_samples_split': 10, 'criterion': 'gini'}. Best is trial 0 with value: 0.8017233767832863.\n",
      "[I 2024-11-30 11:15:27,162] A new study created in memory with name: SVM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimização para o modelo: SVM\n"
     ]
    }
   ],
   "source": [
    "# Otimização de hiperparâmetros com Optuna\n",
    "# Configurando o Callback do MLflow para o Optuna\n",
    "mlflc = MLflowCallback(tracking_uri=mlflow.get_tracking_uri(), metric_name='accuracy')\n",
    "\n",
    "# Definindo funções auxiliares para construção e otimização dos modelos\n",
    "def build_classifier(model_name, params):\n",
    "    if model_name == 'K-NN':\n",
    "        classifier = KNeighborsClassifier(\n",
    "            n_neighbors=params['n_neighbors'],\n",
    "            weights=params['weights'],\n",
    "            algorithm=params['algorithm']\n",
    "        )\n",
    "    elif model_name == 'LVQ':\n",
    "        classifier = NearestCentroid(\n",
    "            metric=params['metric'],\n",
    "            shrink_threshold=params['shrink_threshold']\n",
    "        )\n",
    "    elif model_name == 'Árvore de Decisão':\n",
    "        classifier = DecisionTreeClassifier(\n",
    "            max_depth=params['max_depth'],\n",
    "            min_samples_split=params['min_samples_split'],\n",
    "            criterion=params['criterion']\n",
    "        )\n",
    "    elif model_name == 'SVM':\n",
    "        classifier = SVC(\n",
    "            C=params['C'],\n",
    "            kernel=params['kernel'],\n",
    "            gamma=params['gamma']\n",
    "        )\n",
    "    elif model_name == 'Random Forest':\n",
    "        classifier = RandomForestClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            min_samples_split=params['min_samples_split']\n",
    "        )\n",
    "    elif model_name == 'Rede Neural MLP':\n",
    "        classifier = MLPClassifier(\n",
    "            hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "            activation=params['activation'],\n",
    "            solver=params['solver'],\n",
    "            alpha=params['alpha'],\n",
    "            max_iter=500\n",
    "        )\n",
    "    elif model_name == 'Comitê de Redes Neurais Artificiais':\n",
    "        nn1 = MLPClassifier(\n",
    "            hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "            activation=params['activation'],\n",
    "            solver=params['solver'],\n",
    "            max_iter=500,\n",
    "            random_state=1\n",
    "        )\n",
    "        nn2 = MLPClassifier(\n",
    "            hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "            activation=params['activation'],\n",
    "            solver=params['solver'],\n",
    "            max_iter=500,\n",
    "            random_state=2\n",
    "        )\n",
    "        nn3 = MLPClassifier(\n",
    "            hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "            activation=params['activation'],\n",
    "            solver=params['solver'],\n",
    "            max_iter=500,\n",
    "            random_state=3\n",
    "        )\n",
    "        classifier = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('nn1', nn1),\n",
    "                ('nn2', nn2),\n",
    "                ('nn3', nn3)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "    elif model_name == 'Comitê Heterogêneo (Stacking)':\n",
    "        final_estimator = LogisticRegression(\n",
    "            C=params['C'],\n",
    "            penalty=params['penalty'],\n",
    "            solver=params['solver']\n",
    "        )\n",
    "        estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "            ('svm', SVC(random_state=42)),\n",
    "            ('knn', KNeighborsClassifier())\n",
    "        ]\n",
    "        classifier = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            final_estimator=final_estimator,\n",
    "            cv=5\n",
    "        )\n",
    "    elif model_name == 'XGBoost':\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            subsample=params['subsample'],\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    elif model_name == 'LightGBM':\n",
    "        classifier = LGBMClassifier(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            num_leaves=params['num_leaves'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            subsample=params['subsample']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo {model_name} não reconhecido.\")\n",
    "    return classifier\n",
    "\n",
    "def objective_factory(model_name):\n",
    "    def objective(trial):\n",
    "        params = {}\n",
    "        # Definir os hiperparâmetros para cada modelo\n",
    "        if model_name == 'K-NN':\n",
    "            params['n_neighbors'] = trial.suggest_int('n_neighbors', 1, 30)\n",
    "            params['weights'] = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            params['algorithm'] = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "        \n",
    "        elif model_name == 'LVQ':\n",
    "            params['metric'] = trial.suggest_categorical('metric', ['euclidean', 'manhattan'])\n",
    "            use_shrinkage = trial.suggest_categorical('use_shrinkage', [True, False])\n",
    "            params['shrink_threshold'] = trial.suggest_float('shrink_threshold', 0.0, 1.0) if use_shrinkage else None\n",
    "        \n",
    "        elif model_name == 'Árvore de Decisão':\n",
    "            params['max_depth'] = trial.suggest_int('max_depth', 1, 20)\n",
    "            params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 10)\n",
    "            params['criterion'] = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "        \n",
    "        elif model_name == 'SVM':\n",
    "            params['C'] = trial.suggest_float('C', 0.1, 10.0, log=True)\n",
    "            params['kernel'] = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "            params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        \n",
    "        elif model_name == 'Random Forest':\n",
    "            params['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)\n",
    "            params['max_depth'] = trial.suggest_int('max_depth', 2, 20)\n",
    "            params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        \n",
    "        elif model_name == 'Rede Neural MLP':\n",
    "            params['hidden_layer_sizes'] = trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50,50)])\n",
    "            params['activation'] = trial.suggest_categorical('activation', ['tanh', 'relu'])\n",
    "            params['solver'] = trial.suggest_categorical('solver', ['sgd', 'adam'])\n",
    "            params['alpha'] = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "        \n",
    "        elif model_name == 'Comitê de Redes Neurais Artificiais':\n",
    "            params['hidden_layer_sizes'] = trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50,50)])\n",
    "            params['activation'] = trial.suggest_categorical('activation', ['tanh', 'relu'])\n",
    "            params['solver'] = trial.suggest_categorical('solver', ['sgd', 'adam'])\n",
    "        \n",
    "        elif model_name == 'Comitê Heterogêneo (Stacking)':\n",
    "            params['C'] = trial.suggest_float('C', 0.1, 10.0, log=True)\n",
    "            params['penalty'] = trial.suggest_categorical('penalty', ['l2'])\n",
    "            params['solver'] = trial.suggest_categorical('solver', ['lbfgs'])\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            params['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)\n",
    "            params['max_depth'] = trial.suggest_int('max_depth', 3, 15)\n",
    "            params['learning_rate'] = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "            params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            params['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)\n",
    "            params['num_leaves'] = trial.suggest_int('num_leaves', 31, 150)\n",
    "            params['learning_rate'] = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "            params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modelo {model_name} não reconhecido.\")\n",
    "        \n",
    "        # Treinando no conjunto de treinamento balanceado\n",
    "        # classifier.fit(X_train, y_train)\n",
    "        # # Avaliando no conjunto de validação\n",
    "        # y_pred_val = classifier.predict(X_val)\n",
    "        # acc = accuracy_score(y_val, y_pred_val)\n",
    "        classifier = build_classifier(model_name, params)\n",
    "        \n",
    "        acc = cross_val_score(classifier, X_train, y_train, scoring='accuracy', n_jobs=-1).mean()\n",
    "        return acc\n",
    "    return objective\n",
    "\n",
    "# Executando as Otimizações com Optuna para SVM e Random Forest\n",
    "n_trials = 1  # Número de iterações\n",
    "\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "cv_results = {}\n",
    "# models_to_optimize = ['K-NN', 'Random Forest']\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(f\"Otimização para o modelo: {model_name}\")\n",
    "    study = optuna.create_study(direction='maximize', study_name=model_name, sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective_factory(model_name), n_trials=n_trials, callbacks=[mlflc])\n",
    "    \n",
    "    # Armazenando os melhores hiperparâmetros\n",
    "    best_params[model_name] = study.best_params\n",
    "    \n",
    "    # Criando o classificador com os melhores hiperparâmetros\n",
    "    classifier = build_classifier(model_name, best_params[model_name])\n",
    "    \n",
    "    # Armazenando o classificador otimizado\n",
    "    best_models[model_name] = classifier\n",
    "    \n",
    "    # Armazenando os scores dos folds\n",
    "    acc_scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_results[model_name] = acc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN - Acurácia Média: 0.7505, F1-Score Médio: 0.7975, Recall Médio: 0.9941, ACSA Média: 0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 01:38:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Acurácia Média: 0.9067, F1-Score Médio: 0.8701, Recall Médio: 0.8581, ACSA Média: 0.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/30 01:43:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Modelo  Acurácia  F1-Score    Recall      ACSA\n",
      "0           K-NN  0.750477  0.797460  0.994097  0.753309\n",
      "1  Random Forest  0.906664  0.870065  0.858084  0.906096\n"
     ]
    }
   ],
   "source": [
    "# Avaliando os modelos otimizados\n",
    "# results = []\n",
    "\n",
    "# for model_name, model in best_models.items():\n",
    "#     with mlflow.start_run(run_name=f\"{model_name} - Optuna HPO\"):\n",
    "#         acc, f1, recall, acsa = evaluate_model(model, X_train, y_train, model_name)\n",
    "#         mlflow.log_params(best_params[model_name])\n",
    "#         mlflow.log_metric(\"accuracy\", acc)\n",
    "#         mlflow.log_metric(\"f1_score\", f1)\n",
    "#         mlflow.log_metric(\"recall\", recall)\n",
    "#         mlflow.log_metric(\"acsa\", acsa)\n",
    "#         # Salvar o modelo\n",
    "#         mlflow.sklearn.log_model(model, model_name)\n",
    "#         results.append({\n",
    "#             'Modelo': model_name,\n",
    "#             'Acurácia': acc,\n",
    "#             'F1-Score': f1,\n",
    "#             'Recall': recall,\n",
    "#             'ACSA': acsa\n",
    "#         })\n",
    "\n",
    "# # Converte os resultados em DataFrame\n",
    "# optimized_results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Exibindo os resultados dos modelos otimizados\n",
    "# print(optimized_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN - Acurácia Média: 0.7505, F1-Score Médio: 0.7975, Recall Médio: 0.9941, ACSA Média: 0.7533\n",
      "LVQ - Acurácia Média: 0.5075, F1-Score Médio: 0.6171, Recall Médio: 0.8032, ACSA Média: 0.5109\n",
      "Árvore de Decisão - Acurácia Média: 0.8518, F1-Score Médio: 0.8291, Recall Médio: 0.8499, ACSA Média: 0.8518\n"
     ]
    }
   ],
   "source": [
    "# Avaliando todos os modelos\n",
    "# final_results = []\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     if model_name in best_models:\n",
    "#         # Usar o modelo otimizado\n",
    "#         model = best_models[model_name]\n",
    "#     else:\n",
    "#         # Usar o modelo padrão\n",
    "#         model = model\n",
    "#     acc, f1, recall, acsa = evaluate_model(model, X_train, y_train.values, model_name)\n",
    "#     final_results.append({\n",
    "#         'Modelo': model_name,\n",
    "#         'Acurácia': acc,\n",
    "#         'F1-Score': f1,\n",
    "#         'Recall': recall,\n",
    "#         'ACSA': acsa\n",
    "#     })\n",
    "\n",
    "# # Convertendo para DataFrame\n",
    "# final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# # Exibindo os resultados finais\n",
    "# print(final_results_df)\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{model_name} - Avaliação Final\"):\n",
    "        acc, f1, recall, acsa = evaluate_model(model, X_train, y_train, X_val, y_val, model_name)\n",
    "        mlflow.log_params(best_params[model_name])\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"acsa\", acsa)\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        final_results.append({\n",
    "            'Modelo': model_name,\n",
    "            'Acurácia': acc,\n",
    "            'F1-Score': f1,\n",
    "            'Recall': recall,\n",
    "            'ACSA': acsa\n",
    "        })\n",
    "\n",
    "# Convertendo para DataFrame\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Exibindo os resultados finais\n",
    "print(final_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando as métricas\n",
    "final_results_melted = final_results_df.melt(id_vars='Modelo', value_vars=['Acurácia', 'F1-Score', 'Recall', 'ACSA'], var_name='Métrica', value_name='Valor')\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x='Modelo', y='Valor', hue='Métrica', data=final_results_melted)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Comparação das Métricas dos Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usando os scores de validação cruzada\n",
    "# model_scores = {}\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     if model_name in best_models:\n",
    "#         model = best_models[model_name]\n",
    "#     else:\n",
    "#         model = model\n",
    "#     skf = StratifiedKFold(n_splits=10)\n",
    "#     scores = cross_val_score(model, X_train, y_train.values, cv=skf, scoring='accuracy')\n",
    "#     model_scores[model_name] = scores\n",
    "\n",
    "# # Criando DataFrame com os scores\n",
    "# scores_df = pd.DataFrame(model_scores)\n",
    "\n",
    "# # Teste de Friedman\n",
    "# stat, p = friedmanchisquare(*[scores_df[model] for model in scores_df.columns])\n",
    "# print(f'Estatística de Friedman: {stat}, p-valor: {p}')\n",
    "\n",
    "# # Se p-valor < 0.05, há diferença significativa\n",
    "# if p < 0.05:\n",
    "#     print('Diferença significativa entre os modelos. Realizando teste de Nemenyi.')\n",
    "#     nemenyi = sp.posthoc_nemenyi_friedman(scores_df.values)\n",
    "#     nemenyi_df = pd.DataFrame(nemenyi, index=scores_df.columns, columns=scores_df.columns)\n",
    "#     print(nemenyi_df)\n",
    "# else:\n",
    "#     print('Não há diferença significativa entre os modelos.')\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "metricas = ['Acurácia', 'F1-Score', 'Recall', 'ACSA']\n",
    "\n",
    "for metrica in metricas:\n",
    "    print(f\"\\nAnálise Estatística para a Métrica: {metrica}\")\n",
    "    # Pivotando o DataFrame\n",
    "    pivot_df = cv_results_df.pivot(index='Fold', columns='Modelo', values=metrica)\n",
    "    \n",
    "    # Removendo modelos que não possuem valores (caso haja)\n",
    "    pivot_df = pivot_df.dropna(axis=1, how='any')\n",
    "    \n",
    "    # Aplicando o teste de Friedman\n",
    "    stat, p = friedmanchisquare(*[pivot_df[model] for model in pivot_df.columns])\n",
    "    print(f'Estatística de Friedman: {stat}, p-valor: {p}')\n",
    "    \n",
    "    # Se p-valor < 0.05, há diferença significativa\n",
    "    if p < 0.05:\n",
    "        print('Diferença significativa entre os modelos. Realizando teste de Nemenyi.')\n",
    "        nemenyi = sp.posthoc_nemenyi_friedman(pivot_df.values)\n",
    "        nemenyi_df = pd.DataFrame(nemenyi, index=pivot_df.columns, columns=pivot_df.columns)\n",
    "        print(nemenyi_df)\n",
    "    else:\n",
    "        print(f'Não há diferença significativa entre os modelos para a métrica {metrica}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Custo-Benefício"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_benefit_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name in best_models:\n",
    "        model = best_models[model_name]\n",
    "    else:\n",
    "        model = model\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train.values)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    mem_usage = sys.getsizeof(model)\n",
    "    cost_benefit_results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Tempo de Treinamento (s)': training_time,\n",
    "        'Uso de Memória (bytes)': mem_usage\n",
    "    })\n",
    "\n",
    "# Convertendo para DataFrame\n",
    "cost_benefit_df = pd.DataFrame(cost_benefit_results)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(cost_benefit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o tempo de treinamento\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='Modelo', y='Tempo de Treinamento (s)', data=cost_benefit_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Tempo de Treinamento dos Modelos')\n",
    "plt.show()\n",
    "\n",
    "# Plotando o uso de memória\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x='Modelo', y='Uso de Memória (bytes)', data=cost_benefit_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Uso de Memória dos Modelos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de Estresse dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando o conjunto de teste para o teste de estresse\n",
    "stress_test_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name in best_models:\n",
    "        model = best_models[model_name]\n",
    "    else:\n",
    "        model = model\n",
    "    # Avaliando no conjunto de teste\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    # Cálculo da ACSA\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    acsa_test = np.mean(np.diag(cm) / np.sum(cm, axis=1))\n",
    "    stress_test_results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Acurácia': acc_test,\n",
    "        'F1-Score': f1_test,\n",
    "        'Recall': recall_test,\n",
    "        'ACSA': acsa_test\n",
    "    })\n",
    "\n",
    "# Convertendo para DataFrame\n",
    "stress_test_df = pd.DataFrame(stress_test_results)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(stress_test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussão sobre Limitações e Futuras Melhorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
